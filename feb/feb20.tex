\newpage

\section{Thursday, February 20, 2020}

Today, we'll discuss two more algorithmic problems, both of which have greedy optimal solutions. 

\subsection{Interval Scheduling}

The first problem we'll discuss is known as the \vocab{interval scheduling problem}, which is stated as follows:

\begin{quote}
    Given a pair of parallel arrays $\verb!start[1...N]!$ and $\verb!finish[1...N]!$, call a set of indices $S$ \vocab{compatible} if, for any pair of indices $i, j \in S$, the intervals $\verb!(start[i], finish[i])!$ and $\verb!(start[j], finish[j])!$ are disjoint. Moreover, call a compatible set $S$ \vocab{optimal} if its cardinality is maximal. The goal is to find an optimal set. 
\end{quote}

Why do we care about this problem? For each index $1 \leq k \leq N$, we can interpret the quantity $\verb!start[k]!$ and $\verb!finish[k]!$ as the starting time and ending time of an event. Under this interpretation, our task is to fit as many events as possible into our calendar. \\

There are several algorithms that we can implement that following the greedy heuristic:

\begin{enumerate}
    \item One approach is to always select the next available event that always starts the earliest (i.e. keep on picking $\argmin_{k \in \{1, 2, \ldots N\}} \verb!start[k]!$), and remove $k$ from our set afterwards. This method, however, is not optimal. A counterexample can be generated by considering the case in which the event with the earliest start time is very very long. By accepting this request, we'll miss out on many other events.
    \item A second approach is to keep on picking $\argmin_{k\in \{1, 2, \ldots, N\}}$ $\verb!finish[k] - start[k]!$ and remove the index we picked from our set. While this is better than the other approach, this isn't optimal either.
    \item A third approach is to pick the next request that finishes first (that is, pick $k = \argmin_{k\in \{1, 2, \ldots, N\}} \verb!finish[k]!$) over and over again. This algorithm seems similar to our first idea. Surprisingly, however, this is the optimal solution. 
\end{enumerate}

Some pseudocode illustrating how this procedure works is presented below:

\vspace{1em}
\begin{center}
\line(1,0){400}
\end{center}

\begin{allintypewriter}
\# Input: A set $S$ representing the

\# Output: An optimal solution $A$.

SCHEDULING(S) \string{

\hspace{0.5cm} let A be the empty set.

\hspace{0.5cm} while S isn't empty \string{

\hspace{1cm} let e be the event in S with the smallest finishing time.

\hspace{1cm} add request e to set A.

\hspace{1cm} remove any events that aren't compatible with e from S.

\hspace{0.5cm} \string}

\hspace{0.5cm} return A

\string}

\begin{center}
\line(1,0){400}
\end{center}
\end{allintypewriter}

Next, we'll prove that the set $A$ returned by this algorithm is an optimal solution. 


\begin{proposition}
The set $A$ returned by our algorithm is a compatible set of events.
\end{proposition}
\begin{proof}
On each iteration, we add an event, and we remove any events that \textit{aren't} compatible with the event we just added. Since the compatibility relationship between events is symmetric, we know that we'll never have a pair of incompatible events in $A$. 
\end{proof}

Now we need to show that the set $A$ produced by this algorithm has maximal cardinality. In order to do so, let $\mathcal{O}$ be an optimal set of intervals. We want to show $|\mathcal{O}| = |A|$. \\

In other words, if $A = \{i_1, i_2, \ldots, i_k\}$ and $\mathcal{O} = \{j_1, j_2, \ldots, j_m\}$, then our goal is to show $k = m$. \\

In order to show that this is true, we need to make use of the following lemma:\\

\begin{lemma}
For any indices $r \leq k$, we have $\verb!finish[!i_{r}\verb!]! \leq \verb!finish[!j_r\verb!]!$. 
\end{lemma}
\begin{proof}
For brevity, this proof writes $f(k)$ and $s(k)$ represent $\verb!finish[k]!$ and $\verb!start[k]!$, respectively. \\

When $r = 1$, the statement holds since our algorithm always picks the index $i_1$ corresponding to the event with the minimum finish time. Now suppose $f(i_{r-1}) \leq f(j_{r-1})$. We want to show $f(i_{r}) \leq f(j_{r})$. But this is clearly true since $f(j_{r - 1}) \leq s(j_{r})$ implies $f(i_{r - 1}) \leq s(j_r)$. This means that $j_r$ is in the set $S$ of compatible events at the time when the greedy algorithm chooses $i_r$. Since the greedy algorithm always picks the event with the minimum finish time, we must have $f(i_r) \leq f(j_r)$.
\end{proof}

Lemma $8.2$ means precisely that our greedy algorithm's intervals are finished at least as soon as the corresponding intervals in $\mathcal{O}$. \\

We can now prove our original claim:


\begin{proposition}
The set $A$ returned by our greedy algorithm has maximal cardinality.
\end{proposition}
\begin{proof}
If $A$ doesn't have maximal cardinality, then an optimal set $\mathcal{O}$ must have more requests. In other words, we require $m > k$. Applying our previous lemma with $r = k$, we find $f(i_k) \leq f(j-k)$. But since $m > k$, there exists some request $j_{k+1}$ in $\mathcal{O}$. Since this request starts after the event corresponding to $j_k$ ends, deleting all of the requests that aren't compatible with $i_1, \ldots, i_k$ will still contain $j_{k+1}$. However, this means that the greedy algorithm stops with a request present in set, when it's actually only supposed to stop when $S$ is empty.
\end{proof}

\subsubsection{Extensions: Minimizing Lateness}

Once again, consider the situation in which we have a set of $n$ events that we want to schedule in an interval of time. But now, instead of a start time and a finish time, each event has a \textit{deadline}. We say an event $k$ is \vocab{late} if our finish time is greater than its deadline. Moreover, we define the \textit{lateness} of a late event as the difference between the time at which it was finished and the time of the deadline. The objective of this problem is to minimize the number of late events. \\

The greedy algorithm in this problem is to sort the jobs in increasing order of their deadlines, and schedule them in this order (i.e. we process the events with the earliest deadline first). We will not prove the correctness of this algorithm.



\subsection{Caching}

A \vocab{cache} is a piece of hardware or software that stores data in a special location so that future requests for that data can be served in a high-speed manner. The idea of caching is to store frequently-used values in a special area so that we can access the values in a quick manner. If a value is \textit{not} cached, then we say that the value is stored in \vocab{main memory}. \\

In order to have an effective cache, it should usually be the case that when we're trying to access a piece of data, it's already present in the cache. Today, we'll talk about a cache maintenance algorithm that determines what to keep in the cache and what to toss out of the cache when new data is brought in. \\

Our problem is stated as follows: \\

\begin{quote}
    Let $U$ be a set containing $n$ pieces of data stored in main memory, and let $C$ denote our cache that can hold $k < n$ pieces of memory. Given a sequence of data items $d_1, d_2, \ldots, d_m$ drawn from $U$, we must process them in order and determine which of the $k$ items to keep in the cache. When an item $d_i$ is presented that isn't in $C$, we say a \vocab{cache miss} occurs (we want to minimize these), and we have the option to evict some other data element in $C$ in exchange for $d_i$. Thus, our problem consists of computing the minimum number of cache misses necessary to process our data sequence.
\end{quote}


\begin{example}
[Caching Example]
Suppose $U = \{a, b, c\}$, and our cache size is $k = 2$. Moreover, suppose we are presented with the sequence 
\[
a,b,c,b,c,a,b.
\]
If the cache initially contains items $a$ and $b$, then on the third item in the sequence, we can evict $a$ to bring in $c$, and on the sixth item, we could evict $c$ to bring in $a$. This results in two total cache misses. It can be shown that no solution can have fewer than two cache misses.
\end{example}


\subsection{Farthest in Future Algorithm}

Surprisingly, the solution to the caching problem is fairly short.  When data element $d_i$ needs to be brought into the cache, we should always evict the item that is needed the farthest into the future. This is known as the \vocab{Farthest-in-Future algorithm}, and it was discovered by Belady. \\

We won't prove optimality; however, it's important to take note that that greedy algorithms might not always be obvious (why do we evict the element farthest in the future as opposed to the least frequent element?) 